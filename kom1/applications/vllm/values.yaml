vllm:
  servingEngineSpec:
    runtimeClassName: ""
    modelSpec:
      - name: "gemma"
        repository: "lmcache/vllm-openai"
        tag: "v0.3.0"
        modelURL: "google/gemma-3-27b-it"

        replicaCount: 1
        requestCPU: 4
        requestMemory: "40Gi"
        requestGPU: 1

        pvcStorage: "200Gi"
        vllmConfig:
          enableChunkedPrefill: true
          enablePrefixCaching: false
          maxModelLen: 8192

#        lmcacheConfig:
#          enabled: true
#          cpuOffloadingBufferSize: "20"

        hf_token:
          secretName: "hf-secret"
          secretKey: "TOKEN"

        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/vgpu.present
                operator: "In"
                values:
                  - "true"

    vllmApiKey:
      secretName: "vllm-secret"
      secretKey: "KEY"

    tolerations:
    - key: "node-role.kubernetes.io/gpu"
      operator: "Exists"
