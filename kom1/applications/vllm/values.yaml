vllm:
  servingEngineSpec:
    strategy:
      # We only have one GPU node, so we need to kill exiting deployment first.
      type: Recreate

    runtimeClassName: ""
    modelSpec:
      - name: "mistral"
        repository: "vllm/vllm-openai"
        tag: "v0.10.1.1"
        #repository: "lmcache/vllm-openai"
        #tag: "2025-05-27-v1"

        modelURL: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"

        replicaCount: 1

        requestCPU: 4
        requestMemory: "56Gi"
        requestGPU: 1

        # Re-enabled: Temporary workaround for scaling up to more GPU's (currently NFS required to have RWM)
        pvcStorage: "200Gi"

        vllmConfig:
          enableChunkedPrefill: true
          enablePrefixCaching: true
          maxModelLen: 16384
          extraArgs: ["--tokenizer-mode", "mistral", "--config_format", "mistral", "--load_format", "mistral"]

        lmcacheConfig:
          enabled: false
          cpuOffloadingBufferSize: "20"

        hf_token:
          secretName: "hf-secret"
          secretKey: "TOKEN"

        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/vgpu.present
                operator: "In"
                values:
                  - "true"

    vllmApiKey:
      secretName: "vllm-secret"
      secretKey: "KEY"

    tolerations:
    - key: "node-role.kubernetes.io/gpu"
      operator: "Exists"

  routerSpec:
    repository: "lmcache/lmstack-router"
    tag: "latest"
    imagePullPolicy: "Always"
